{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___ Libraries ___\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.stats import entropy\n",
    "from scipy.special import kl_div\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import community as community_louvain # For Louvain community detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___ Read data ___\n",
    "data_path = r'C:\\Users\\cayde\\Desktop\\Grad_School_stuff\\DataBaseManagement\\Project\\Analysis_Scripts\\data\\mal-api-2019\\all_analysis_data.txt'\n",
    "labels_path = r'C:\\Users\\cayde\\Desktop\\Grad_School_stuff\\DataBaseManagement\\Project\\Analysis_Scripts\\data\\mal-api-2019\\labels.txt'\n",
    "\n",
    "# Read the data from all_analysis_data.txt and labels.txt\n",
    "with open(data_path, \"r\") as f:\n",
    "    all_traces = f.read().split('\\n')[:-1]  # Array of all untokenized trace documents\n",
    "\n",
    "with open(labels_path, \"r\") as g:\n",
    "    all_labels = g.read().split('\\n')[:-1]  # Remove last blank newline from list\n",
    "\n",
    "# Shuffle and limit data and labels to a smaller subsection for testing\n",
    "all_traces, all_labels = shuffle(all_traces, all_labels, random_state=42)\n",
    "all_traces = all_traces[:100]\n",
    "all_labels = all_labels[:100]\n",
    "\n",
    "# Generate trace document names \n",
    "tracedoc_names = {i: i for i in range(len(all_traces))} \n",
    "\n",
    "# Dictionary of trace names and corresponding traces \n",
    "name_trace_dict = {name: trace for name, trace in zip(tracedoc_names.values(), all_traces)}\n",
    "\n",
    "# List of trace document names \n",
    "trace_document_name_list = list(name_trace_dict.keys())\n",
    "\n",
    "# Sorted distinct set list of labels\n",
    "distinct_labels = sorted(set(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___ Functions ___ \n",
    "\n",
    "# Calculate similarity matrix using Cosine Similarity\n",
    "def SM_cosine(documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n",
    "    cosine_matrix = cosine_similarity(feature_matrix)  # Cosine similarity between all document vectors \n",
    "    return cosine_matrix;\n",
    "\n",
    "# Invert similarity matrix values for Cosine MST\n",
    "def inverted_matrix(similarity_matrix): \n",
    "    return [[9999 if y == 0 else 1/y for y in x] for x in similarity_matrix]\n",
    "\n",
    "# Calculate inverted similarity matrix using Jaccard Similarity\n",
    "def SM_invjaccard(documents):\n",
    "    matrix = []\n",
    "    for x in documents:\n",
    "        temp = []\n",
    "        for y in documents:\n",
    "            a = set(x.split(' '))  # Tokenize document x\n",
    "            b = set(y.split(' '))  # Tokenize document y\n",
    "            c = a.intersection(b)  # Find intersection between two trace documents: x, y \n",
    "            j = float(len(c)) / (len(a) + len(b) - len(c))  # Jaccard calculation\n",
    "            \n",
    "            temp.append(9999 if j == 0 else float(1/j))  # Inverse jaccard values\n",
    "        matrix.append(temp)\n",
    "    return matrix;\n",
    "     \n",
    "# Generate a graph for similarity between trace documents and find MST\n",
    "def networkX_graph(matrix, labels):\n",
    "    G = nx.from_numpy_matrix(np.array(matrix))  # Create a graph from the similarity matrix\n",
    "    H = nx.relabel_nodes(G, labels)  # Label nodes with names of each trace\n",
    "    mst = nx.algorithms.tree.mst.minimum_spanning_tree(H)  # Generate a minimum spanning tree of similarity matrix\n",
    "    return mst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL FAMILIES: FAMILY LABEL AND TRACES THEY CONTAIN \n",
    "\n",
    "original_families = []  # Trace document names belonging in each original family\n",
    "all_original_fam_names = []  # List of all names\n",
    "\n",
    "# Limiting families analyzed to smaller number of families\n",
    "distinct_labels = distinct_labels[:2]\n",
    "\n",
    "for i in distinct_labels:  # For each distinct family label\n",
    "    temp = [trace_document_name_list[j] for j in range(len(all_labels)) if all_labels[j] == i]\n",
    "    original_families.append(temp)\n",
    "    all_original_fam_names.extend(temp)\n",
    "     \n",
    "original_families_traces_untokenized = [[name_trace_dict[y] for y in x] for x in original_families]    \n",
    "\n",
    "# ALL SELECTED FAMILY (ONE, TWO, OR THREE+ FAMILIES) TRACES, LABELS, AND TRACE NAMES\n",
    "all_sel_fam_traces = []\n",
    "all_sel_fam_labels = []\n",
    "all_sel_fam_names = {}\n",
    "\n",
    "for x in range(len(original_families_traces_untokenized)):\n",
    "    for y in original_families_traces_untokenized[x]:\n",
    "        all_sel_fam_traces.append(y)\n",
    "        all_sel_fam_labels.append(x)\n",
    "\n",
    "all_sel_fam_names = {x: name for x, name in enumerate(all_original_fam_names)} \n",
    "        \n",
    "all_traces_list = [all_sel_fam_traces, all_sel_fam_labels, all_sel_fam_names]\n",
    "\n",
    "# SAVE Original Family Trace Document Data\n",
    "np.savetxt(\"output/Original_traceDocs.csv\", all_traces_list[0], delimiter=\",\", fmt='%s')\n",
    "np.savetxt(\"output/Original_labels.csv\", all_traces_list[1], delimiter=\",\", fmt='%.3e')\n",
    "np.savetxt(\"output/Original_traceNames.csv\",all_traces_list[2], delimiter=\",\", fmt='%.3e')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVERTED JACCARD MATRIX \n",
    "ismj = SM_invjaccard(all_sel_fam_traces)    \n",
    "\n",
    "# SAVE Inverted Jaccard Matrix\n",
    "SMj = np.asarray(ismj)\n",
    "np.savetxt(\"output/SMj.csv\", SMj, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVERTED GRAPH\n",
    "gij = networkX_graph(ismj, all_sel_fam_names)\n",
    "\n",
    "# SAVE Minimum Spanning Tree Nodes and Edges\n",
    "MSTjnodes = np.asarray(list(gij.nodes()))\n",
    "np.savetxt(\"output/MSTjNodes.csv\", MSTjnodes, delimiter=\",\", fmt='%.0f')\n",
    "\n",
    "MSTjedges = np.asarray(list(gij.edges()))\n",
    "np.savetxt(\"output/MSTjEdges.csv\", MSTjedges, delimiter=\",\", fmt='%.0f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Families created from MST and Girvan Newman \n",
    "def all_traces_clusters_girvan_newman(G, name_trace_dict, maxgroups):   \n",
    "    gn = nx.algorithms.community.centrality.girvan_newman(G) \n",
    "\n",
    "    # Find all iterations with the desired number of groups\n",
    "    sets_of_cluster = next(x for x in gn if len(x) == maxgroups)\n",
    "\n",
    "    list_of_cluster = list(sets_of_cluster) # list of communities and tracedocs within them\n",
    "    target_labels = list(range(len(sets_of_cluster))) # list of distinct communities detected\n",
    "\n",
    "    all_traces, all_labels, all_traces_names = [], [], []\n",
    "    for idx, cluster in enumerate(list_of_cluster):\n",
    "        for name in cluster:\n",
    "            name = int(name)\n",
    "            all_traces_names.append(name)\n",
    "            all_traces.append(name_trace_dict[name])\n",
    "            all_labels.append(idx)\n",
    "\n",
    "    print(\"\\nGIRVAN NEWMAN COMMUNITY DETECTION FAMILIES: \\n\")\n",
    "    for label in target_labels:\n",
    "        print(f\"\\n\\nGroup {label}:  \", end=\" \")\n",
    "        print(\", \".join(str(name) for idx, name in enumerate(all_traces_names) if all_labels[idx] == label))\n",
    "\n",
    "    return [all_traces, all_labels, all_traces_names]\n",
    "\n",
    "\n",
    "# Families created from MST and Louvain \n",
    "def all_traces_clusters_louvain(G, name_trace_dict):\n",
    "    lv = community_louvain.best_partition(G)\n",
    "\n",
    "    lv_traces = list(lv.keys())\n",
    "    lv_groups = list(lv.values())\n",
    "    lv_distinct_groups = sorted(set(lv_groups))\n",
    "\n",
    "    lv_families  = [[trace for idx, trace in enumerate(lv_traces) if group == lv_groups[idx]] for group in lv_distinct_groups]\n",
    "    \n",
    "    print(\"\\n\\nLOUVAIN COMMUNITY DETECTION FAMILIES: \\n\")\n",
    "    all_traces, all_labels, all_traces_names = [], [], []\n",
    "    for idx, family in enumerate(lv_families):\n",
    "        print(f\"\\nGroup {idx}: \", family)\n",
    "        \n",
    "        for name in family:\n",
    "            name = int(name)\n",
    "            all_traces.append(name_trace_dict[name])\n",
    "            all_traces_names.append(name)\n",
    "            all_labels.append(idx)\n",
    "\n",
    "    return [all_traces, all_labels, all_traces_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GIRVAN NEWMAN COMMUNITY DETECTION FAMILIES: \n",
      "\n",
      "\n",
      "\n",
      "Group 0:   65, 69, 6, 7, 45, 17, 19, 51, 85, 22, 23\n",
      "\n",
      "\n",
      "Group 1:   73, 15, 48, 20, 55, 60, 30\n",
      "\n",
      "\n",
      "LOUVAIN COMMUNITY DETECTION FAMILIES: \n",
      "\n",
      "\n",
      "Group 0:  [65, 7, 23, 85]\n",
      "\n",
      "Group 1:  [20, 55, 15, 30, 60, 73]\n",
      "\n",
      "Group 2:  [6, 17, 22, 48]\n",
      "\n",
      "Group 3:  [19, 45, 51, 69]\n"
     ]
    }
   ],
   "source": [
    "# JACCARD MST GIRVAN NEWMAN \n",
    "gnj_all_traces = all_traces_clusters_girvan_newman(gij, name_trace_dict, len(distinct_labels))\n",
    "\n",
    "# Save Jaccard Girvan Newman Communities \n",
    "np.savetxt(\"output/GNJ_traceDocs.csv\", gnj_all_traces[0], delimiter=\",\", fmt='%s')\n",
    "np.savetxt(\"output/GNJ_labels.csv\", gnj_all_traces[1], delimiter=\",\", fmt='%.3e')\n",
    "np.savetxt(\"output/GNJ_traceNames.csv\", gnj_all_traces[2], delimiter=\",\", fmt='%.3e')\n",
    "\n",
    "\n",
    "# JACCARD MST LOUVAIN \n",
    "lvj_all_traces = all_traces_clusters_louvain(gij, name_trace_dict)\n",
    "\n",
    "# Save Jaccard Louvain Communities\n",
    "np.savetxt(\"output/LVJ_traceDocs.csv\", lvj_all_traces[0], delimiter=\",\", fmt='%s')\n",
    "np.savetxt(\"output/LVJ_labels.csv\", lvj_all_traces[1], delimiter=\",\", fmt='%.0f')\n",
    "np.savetxt(\"output/LVJ_traceNames.csv\", lvj_all_traces[2], delimiter=\",\", fmt='%.0f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
